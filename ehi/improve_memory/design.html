<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Cognitive science and imaginative design</title>
    <link rel="stylesheet" href="style.css">
  </head>

  <body>

    <div id="header">
      <h1>Cognitive science and imaginative design</h1>
      <p>
	<a href="http://michaelnielsen.org">Michael Nielsen</a> &nbsp; / &nbsp;	December 2016
      </p>
    </div>

    <div id="container">
      <span class="marginnote">
	Rough working notes, based on a discussion with Caitlin
	Sikora, Katherine Ye, Nicky Case, Xavier Snelgrove, and Yan
	Zhu.  Supported by <a href="http://ycr.org">Y Combinator
	Research</a>, based on work begun at
	the <a href="http://recurse.com">Recurse Center</a>.
      </span>

      <p>
	Much of my research is about prototyping tools to extend human
	intelligence.  Many such tools originated in insights from
	cognitive science.  For example, in
	a <a href="index.html">recent essay</a> I reviewed tools to
	augment long-term memory.  Those tools were inspired by more
	than a century of work on memory by cognitive scientists.
      </p>

      <p>
	Such examples make it tempting to think of the scientific
	literature on cognitive science as a potential gold mine of
	ideas for new cognitive technologies.
      </p>

      <p>
	I think it is a gold mine of ideas.  But I also think it's
	easy to use cognitive science in the wrong way.  So in this
	note I describe a few seductive-but-dangerous approaches to
	the cognitive science literature.  My intent isn't to be a
	curmudgeon.  Rather, it's to understand the mistakes that can
	be made, in order to develop a healthy, creative approach.
      </p>

      <p>
	Let me begin with the &ldquo;studies have shown&rdquo; trope.
      </p>

      <p>
	There's a genre of &ldquo;science&ndash;-based writing that
	uses this trope.  The writing could be on any subject: the
	benefits of more and better sleep, the benefits of meditation,
	or, heaven forfend, on <a href="index.html">the benefits of
	spaced repetition systems for long-term memory</a>.  The
	author claims their work is based on <em>Scientific!
	Studies!</em> of their subject.  Every few paragraphs the
	author drops in a handful of references to scientific papers.
	They will claim &ldquo;studies show one hour more extra sleep
	improves your ability to concentrate [or whatever]&rdquo;, and
	cite 2 or 3 papers.
      </p>

      <p>
	As an author, this is a seductive approach.  It has many
	points in its favour: it's a pretty easy way of serving a very
	useful function, giving the reader an entry-point into the
	scientific literature.  And the conclusions aren't always
	wrong.  
      </p>
      
      <p>
	Unfortunately, the approach also has many problems.  The basic
	  problem is that <strong>peer-reviewed scientific papers are
	  frequently wrong</strong>.  The mere fact that you can find
	  a paper or three to support a conclusion means quite little.
	  Maybe 20 studies have been done to understand how extra
	  sleep relates to concentration.  And the author quotes just
	  3 studies, which happen to be the 3 studies most sympathetic
	  to the conclusion need for their narrative.
      </p>

      <p>
	This sounds malicious, but it's not.  The powerful forces of
	confirmation bias and motivated reasoning push all of us in
	this direction.  It's human nature to be more skeptical of
	papers whose conclusions disagree with what you believe; and
	for like-minded friends and colleagues to share and talk
	unskeptically about papers whose conclusions you like. And so
	an intelligent author can, without malice, end up with a very
	distorted view of what is known. Naturally, they then pass
	this on to their readers.
      </p>
	
      <p>
	Part of the problem is that <strong>it's easy to
	  over-generalize from the results of a study</strong>.  So
	  maybe their are some well-conducted studies of the effect of
	  sleep on concentration.  But maybe the studies only apply to
	  concentration on a particular very special kind of task?  Or
	  maybe the size of the effect depends strongly on the time of
	  day, and it just happened the experiments were done at the
	  time that maximized the effect?  Maybe at other times of
	  day, more sleep actually hinders concentration, rather than
	  helps?  Or maybe the effect turns out to be strongest for
	  people aged 20-50, and tails off before and after; the
	  experiment only involved people in that age range, and so
	  didn't notice this.  You happen to be 67, so the results are
	  not use to you.  Or maybe the effect is much stronger for
	  men, and you're a woman?  Or maybe the effect only holds for
	  people with some particular gene common in the country where
	  the research was done, but less common in your country?  In
	  each case, a study that looks like it supports the idea that
	  sleep-improves-concentration in fact shows something much
	  narrower.  You can multiply alternate hypotheses <em>ad
	  infinitum</em>.
      </p>

      <p>
	I'm making this point in the abstract, using a made-up
	example.  But I've literally had a person scream angrily at me
	after: (a) they made a scientific claim; (b) they backed it up
	with a few citations; and (c) I (quite mildly!) expressed
	reserve about the results of those papers, and wouldn't budge
	from that reserve.
      </p>

      <p>
	Being screamed at was unusual.  But it's nonetheless extremely
	common for people to greatly over-estimate the strength of
	evidence in a paper.  After expressing reserve, I've often
	been told that I'm just being the stereotypical scientist,
	overly focused on details, not seeing the big picture. I've
	had this from people in government: &ldquo;we need to get on
	with things, so we need certainty&rdquo;.  Translated: we need
	to fool ourselves that we understand, so we can provide
	plausible justifications for our superiors and the electorate.
	From people in business I've heard the abbreviated form:
	&ldquo;we need to get on with things&rdquo;.  Which is fine
	and good, but it's likely better to find ways of getting on
	with things while making do with weak evidence, rather than
	pretending the evidence is strong.  And I've heard it from
	journalists, often in unintentionally hilarious ways:
	&ldquo;we need surety, to ensure the public is
	well-informed&rdquo;.  Translated: it's better to fake
	certainty rather than to honestly present our state of
	ignorance.
      </p>

      <p>
	The underlying problem, in the case of cognitive science, is
	that we hardly know anything.  In another essay, I reviewed
	the used of spaced repetition to build systems to assist
	long-term memory.  Spaced repetition is one of the best
	studied effects in cognitive science.  More than a thousand
	studies have been done, using many techniques, across a huge
	number of domains.  The effect sizes are large, the effect
	seems extremely robust.
      </p>

      <p>
	And yet.
      </p>

      <p>
	We don't know how memories are formed &ndash; that is, we
	don't understand the exact changes that occur in the brain, or
	what triggers them.  We don't know what causes memories to
	fade.  We don't understand why some people have excellent
	memories, and other people do not.  Indeed, we even keep
	discovering new ways in which people form memories: it's
	relatively recently that we've discovered things like the
	distinction between <em>episodic</em> memory (for particular
	events that happened in our life), and <em>semantic</em>
	memory (for XXX).  No doubt many more such distinctions remain
	to be discovered. In other words, our understanding of memory
	is dreadfully incomplete.  We perhaps understand it about as
	well as the Ancient Greeks understood chemistry.  And so even
	while spaced repetition is about as good as it gets in
	cognitive science, that means we're still very ignorant.
      </p>

      <p>
	Marvin Minsky has said*<span class="marginnote">* Quote in
	  Stewart Brand's book &ldquo;The Media Lab&rdquo;
	  (1987).</span> &ldquo;We're in the thousand years between no
	  technology and all technology.  You can read what your
	  contemporaries think, but you should remember they are
	  ignorant savages.&rdquo;.  Excusing his antiquated language,
	  I think that's exactly right.
      </p>

      <p>
	What's the solution?
      </p>

      <p>
	One approach, which has been taken by many in the academic HCI
	field, has been to regrd human-computer interaction as a sort
	of extension of cognitive science.
      </p>

      <p>
	For instance, there are many studies of the impact of word
	processing on writing.

	a large number of 

	In an earlyFor Summary: In an early study, Card,
	Robert, and Keenan compared the use of the Bravo word
	processor with pen and paper.  They found the word processor
	led users to make many more modifications to their texts than
	pen and paper.  They found no statistically significant
	differences in fluency, style, or in several other axes.  For
	instance, in 1984, XXX -- typical study design, and study
	result.  Give a second example.
      </p>

      <p>
	This is useful work.  It of course has the same challenges as
	I described earlier, for cognitive science.  But, bit by bit,
	we can accumulate various forms of weak evidence, and use
	those to build up understanding.
      </p>

      <p>
	However, there's a much bigger underlying issue: imaginative
	design is not a science.
      </p>

      <p>
	The success of the great interface designers didn't lie in
	their ability to understand more about human cognition.
	Rather, it lay in their ability to conjure up new
	representations and new operations.  The invention of writing,
	the invention of the alphabet, of maps, of numerals, of screen
	cursors, of the search box, of hyperlinks, and of windowing
	systems: all these, and so many more, did not come out of
	cognitive science.  In short, they came out of the free
	invention of the human mind.
      </p>

      <p>
	In my opinion, the right approach to bring to this is what we
	might call <em>imagination first</em>.  This means pushing the
	limits of our representations and operations.  It means
	developing new modes of interaction.  The cognitive science
	literature can, in this vein, be viewed as a source of
	stimulating ideas. But it shouldn't be a straitjacket,
	constraining exploration. We've barely begun to explore what
	interfaces are, and so it's important to have people who are
	boldly exploring.
      </p>

      <p>
	This imagination-first mode is very different to how a
	cognitive scientist works.  Consider the design of a memory
	system like Anki.  For a cognitive scientist, this poses many
	fascinating research problems, unpacking the right way to
	choose the intervals between study.  This question could
	easily occupt the entire professional life.  By contrast, a
	system builder must gloss over the question.  Rather than
	making the &ldquo;right&rdquo; choice, they must use available
	knowledge to make a &ldquo;pretty good&rdquo; choice.
	Ideally, however, they'll understand that the choice may need
	later revision.
      </p>


      <p>
	You might reasonably ask whether or not one person can do
	both?  However, I think that, in practice, the roles are too
	large to be filled by any one person.  What you want is groups
	of people to devote themselves primarily to one or the other,
	over long periods of time. Some to pushing the boundaries of
	what an interface is.  And others to studying the impact of
	extant interfaces.  And, hopefully, much conversation (and
	perhaps sometimes collaboration) between the two groups.
      </p>
      
      <p>
	This, incidentally, sheds some light on what might otherwise
	appear a rather strange fact.
      </p>

      <p>
	I mentioned that the ideas of spaced repetition have been
	developed by cognitive scientists since 1885.  More than a
	thousand (XXX) studies have been done.  And yet cognitive
	scientists put remarkably little work into applying that work.
	The best known systems are perhaps Anki, by the programmer
	Damien Elmes, SuperMemo, by the programmer Piotr Wozniak, the
	Leitner System, by the science journalist Sebastian Leitner,
	and the Pimsleur system, by the French teacher Paul Pimsleur.
	What is perhaps most notable is that none of these people is a
	cognitive scientist.  Rather, all may be regarded as
	enthusiastic amateurs, who used the results of cognitive
	science to build their systems.
      </p>

      <p>
	So, what should we learn from all this?  Let me sum up.  One
	is that we will put design and bold exploration first.  We'll
	acknowledge that to build systems we oftentimes need to make
	choices, even when the underlying basis is poorly understood.
	And sometimes it's worth just trying something wild.  Early
	studies of word processing were not promising.  And yet I
	think that all told, the invention of word processing is one
	of the great inventions ever made.
      </p>

      <p>
	Rhetorically, in terms of these essays, this suggests taking a
	soft approach.  I plan to quickly and rather superficially
	read a large number of papers, looking for inspiration for
	ideas, and not necessarily engaging deeply.  So, rather than
      </p>
	
    <p>
      <strong>If you want to understand what a result says, and how
	strong the evidence is, you need to read the whole study in
	detail:</strong>  
    </p>

    <p>
      <strong>If you're looking for design inspiration, it's fine to
      </strong>
    </p>

      Kellogg's
      
	people to get quite heated.  One extremely common confusion is
	to think that it means I think the result is wrong, or that
	the opposite is true.  It merely means I think the evidence is
	weak, and that therefore we only know a little, and confidence
	is not justified.
      </p>


                        <p>
	The most egregious use comes from authors who use this
	approach as a sort of rhetorical strategy of dropping the
	authority of <strong>SCIENCE</strong> on the reader's head,
	and daring them to disagree.  There's an implied notion of
	&ldquo;if you disagree with this, you either need to come up
	with a more evidence-based rebuttal, or you

	


      <p>
	I think there's two broad classes of solution.  One is to dig
	down, down, down, to understand in-depth what was actually
	done in an
      </p>
      
      
	
      ZZZ

    

      &ldquo;We need to be evidence-based:&rdquo; The NYT story.
      
      How should we use cognitive science to fit into the design of
      systems to extend human intelligence?

      There are multiple seductive approaches:

      + We should take an "evidence-based approach".  Greg Wilson.
      The idiots who
      
      + It should all be based on science.  The NYT flossing story.

      + People who simply cite peer-reviewed studies.
      
      
      How does cognitive science fit into the design of systems to
      extend human intelligence?

      They're kind of gambits

      One approach: it's all based on science!  The NYT story.  A few
      months ago, there was a story in XXX, pointing out that the
      evidence in favour of flossing one's teeth .  Immediately, I saw
      many people on

      One day, perhaps, we'll 

    <h2>Cognitive science and imaginative design</h2>


    No-one thinks they're guilty of

    Lack of a certain kind of integrity.  I'll give you an example.
    FOr many years, I worked full-time as an advocate for open
    science.  Many of my colleagues.  Some of thoese people liked to
    cite studies showing 
    
    <p>
      Other problems also arise.  Maybe all 20 studies are sympathetic
      to the author's conclusion.  But the reason for this may well be
      that the entire community doing the studying is too-sympathetic
      to one particular point of view.
    </p>

    <p>
      For example, early in her career the writer and psychologist
      Susan Blackmore investigated parapsychology.  There's a
      surprisingly large and active community of people attempting
      scientific-appearing study of parapsychology, and finding
      positive results.  Blackmore*<span class="marginnote">* Susan
      Blackmore, <a href="assets/Blackmore1987.pdf">The Elusive Open
      Mind: Ten Years of Negative Research in Parapsychology</a>
      (1987).</span>  describes years of frustration at obtaining
      negative results, and the response of her peers:
    </p>

    <blockquote>
      Why did this study also fail? I had used trained subjects in
      psi-conducive conditions and a method others had found
      successful. The ultimate suggestion of most parapsychologists
      was that it was an experimenter effectâ€”more than that, it was a
      psi-mediated experimenter effect. That is, either I was using my
      own negative psi or I had some kind of personality defect, or
      defect in belief, that suppressed the psi of other people. I was
      a psi-inhibitory experimenter, so that whatever I did I would
      always get negative results. I began to get the feeling that I
      had some creeping sickness. I was a failure, a reject; there was
      something in me that suppressed the true spiritual nature of
      other people. I tried not to let it upset me, but I must admit
      that there is something terribly unflattering about being
      labeled &ldquo;psi-inhibitory&rdquo;!
    </blockquote>

    <p>
      Parapsychology is an extreme case, but it's not that extreme.
      In XXX, some of the most celebrated findings of psychology have
      come under attack.  Consider the phenomenon of priming.  
    </p>
      
    <p>
      A second set of problems involves the appalling norms around
      reporting in cognitive science.  Suppose a paper takes a group
      of 20 people, mounts an intervention, and (say) finds that
      certain test scores go up.  There's then a kind of shorthand way
      this is reported: &ldquo;such-and-such an intervention causes
      physics scores to go up!&rdquo;
    </p>

    <p>
      Well, no.
    </p>



    Approaches: 1. It should all be based on SCIENCE.  The flossing
    approach.  No. In design, it's worthwhile adopting some of the
    stories we have, and proceeding on that basis.
    

    Style 1: The kind that thinks it should all be justified by
    cognitive science.  We hardly know anything about cognitive
    science.

    Our style: It will be to simultaneously keep in mind that we
    hardly k

    There's a kind of person who sneers at &ldquo;unscientific&rdquo;
    self-help books.  But, in fact, many of the
    &lsquo;scientific&rdquo; books are even worse, relying on a veneer
    of respectability provided by the fact that they are &ldquo;backed
    by scientific studies&rdquo;.

    The only resolution I know of to these problems is to get in
    fairly deeply to the details.  What was actually done?
    

      So I think the right rules are this: We're going to put
      imaginative design first.  We will, however, use papers from the
      cognitive science literature as a source of inspiration.

      + We're going to put imaginative design first.

      + Use papers for inspiration.

      + If you don't read for detail, you really don't know much.

    As far as I can tell, the use of phrases like &ldquo;the science
    says&rdquo; are often a tell that the writer doesn't have a clue
    what they're talking about.  It's a way of bludgeoning the reader,
    and it
	
    <h3>Acknowledgements</h3>


    <h3>Citation</h3>

    <p>
      In academic work, please cite this essay as: <em>Michael
	Nielsen, &ldquo;Tools to improve long-term memory&rdquo;,
	available
	at <a href="http://cognitivemedium.com/tat/index.html">http://cognitivemedium.com/ehi/improve_memory/index.html</a>
	(2016)</em>.
    </p>

    <p>
      In non-academic work, I'd appreciate it (and it would help me
      out) if you could give me a shout-out, too!
    </p>

    </div>    

    <div id="footer">
      This work is licensed under a <a rel="license"
	 href="http://creativecommons.org/licenses/by/4.0/">Creative
	 Commons Attribution 4.0 International License</a>.  This
	 means you're free to copy, share, and build on the work,
	 provided you attribute it appropriately.  Please click on the
	 following license link for details: <a rel="license"
	 href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative
	 Commons License" style="border-width: 0; height: 21px;"
	 src="https://i.creativecommons.org/l/by/4.0/88x31.png"/></a>
    </div>
<!--    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-44208967-4', 'auto');
      ga('send', 'pageview');
    </script>
-->

  </body>
</html>  






